Command:
FatTail.py
System:
biag-gpu4.cs.unc.edu
Python:
/playpen-raid1/tgreer/anaconda3/envs/tensorflow/bin/python
Git Hash:
28f1ddf
Uncommitted changes:
diff --git a/tensorflow/FatTail.py b/tensorflow/FatTail.py
index 1b04bb2..93a6311 100644
--- a/tensorflow/FatTail.py
+++ b/tensorflow/FatTail.py
@@ -13,7 +13,7 @@ import matplotlib.pyplot as plt
 def training_generator():
     while True:
         inp = next(gen).numpy().astype(np.float32) / 255.
-        yield (inp[:12, :, :, [2, 1, 0]], inp[:12, :, :, [6, 5, 4]]), inp[:12, :, :, [6, 5, 4]]
+        yield (inp[:, :, :, [2, 1, 0]], inp[:, :, :, [6, 5, 4]]), inp[:, :, :, [6, 5, 4]]
 
 # Detect hardware
 try:
@@ -40,7 +40,7 @@ else:
   
 print("Number of accelerators: ", strategy.num_replicas_in_sync)
 
-BATCH_SIZE = 12 * strategy.num_replicas_in_sync # Gobal batch size.
+BATCH_SIZE = 16 * strategy.num_replicas_in_sync # Gobal batch size.
 # The global batch size will be automatically sharded across all
 # replicas by the tf.data.Dataset API. A single TPU has 8 cores.
 # The best practice is to scale the batch size by the number of
@@ -62,6 +62,7 @@ with strategy.scope():
 # print model layers
 model.summary()
 
+#model.load_weights("results/car_lilbatch/epoch15/model_weights.tf")
 # set up learning rate decay
 lr_decay = tf.keras.callbacks.LearningRateScheduler(
     lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,
diff --git a/tensorflow/car_videos.py b/tensorflow/car_videos.py
index 318d2e8..728803d 100644
--- a/tensorflow/car_videos.py
+++ b/tensorflow/car_videos.py
@@ -11,7 +11,7 @@ SHUFFLE_SIZE = 600
 
 BATCH_SIZE=64
 
-video_dir = "/playpen1/tgreer/IdiotsInCars/data256/"
+video_dir = "/playpen-nvme/tgreer/DashCamVideos/"
 def framePacket():       
 
     available_videos = os.listdir(video_dir)
@@ -57,8 +57,8 @@ def grabShufflePutStep(inp, out):
         bigPacket = [inp.get() for _ in range(SHUFFLE_SIZE)]
         x = torch.cat(bigPacket, 0)
         indices = torch.randperm(SHUFFLE_SIZE * 64)
-        for i in range(SHUFFLE_SIZE * 4):
-            out.put(x[indices[i * 16 : (i + 1) * 16]])
+        for i in range(SHUFFLE_SIZE):
+            out.put(x[indices[i * 64: (i + 1) * 64]])
 
 def grabShufflePut(inp, out):
     while True:
@@ -67,7 +67,7 @@ def threadedProvide():
     
     packetQueue = multiprocessing.Queue(SHUFFLE_SIZE)
     packetProcesses = [None for _ in range(4)]
-    for i in range(2):
+    for i in range(3):
         packetProcesses[i] = multiprocessing.Process(target=putFramePacketsOnQueue, args=(packetQueue,), daemon=True)
         packetProcesses[i].start()
     shuffledQueue = multiprocessing.Queue(SHUFFLE_SIZE // 6)
diff --git a/tensorflow/models.py b/tensorflow/models.py
index 771cf8b..87c0458 100644
--- a/tensorflow/models.py
+++ b/tensorflow/models.py
@@ -75,8 +75,9 @@ def make_model(clip_value, SIDE_LENGTH, FEATURE_LENGTH=128, model = None):
     loss = tf.keras.layers.Lambda(lambda var: tf.math.reduce_sum(var, axis=-1))(loss)
 
 
-    loss = tf.keras.layers.Lambda(lambda var: tf.clip_by_value(var, 0, clip_value), name="clip")(loss)
 
+    #loss = tf.keras.layers.Lambda(lambda var: tf.clip_by_value(var, 0, clip_value), name="clip")(loss)
+    loss = tf.keras.layers.Lambda(lambda var: tf.math.log(tf.clip_by_value(var, 0.000001, 1)), name="log")(loss)
     def fmapICON_clamp_loss(tensor, nonsense):
     
       return -tf.math.reduce_mean(nonsense)
