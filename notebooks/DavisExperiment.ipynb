{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9a934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import footsteps\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def DavisEval(model, name):\n",
    "    davis_path = \"data_storage/DAVIS/\"\n",
    "    output_path = footsteps.output_dir + name + \"/\"\n",
    "    #os.mkdir(output_path)\n",
    "    with open(davis_path + \"/ImageSets/2017/val.txt\", \"r\") as f:\n",
    "        sequences = f.readlines()\n",
    "    for sequence in tqdm.tqdm(sequences[4:5]):\n",
    "        sequence = sequence[:-1]  # strip newline\n",
    "        sequence_out_path = output_path + sequence + \"/\"\n",
    "        sequence_img_path = davis_path + \"JPEGImages/480p/\" + sequence + \"/\"\n",
    "        with open(davis_path + \"Annotations/480p/\" + sequence + \"/00000.png\", \"rb\") as handle:\n",
    "          first_annotation = np.array(\n",
    "            Image.open(handle)\n",
    "          )\n",
    "\n",
    "        #os.mkdir(output_path + sequence)\n",
    "        with open(sequence_img_path + \"00000.jpg\", \"rb\") as handle:\n",
    "          first_image = np.array(Image.open(handle))\n",
    "\n",
    "        prev_image = first_image\n",
    "        prev_annotation = first_annotation\n",
    "\n",
    "        for i in range(1, len(os.listdir(sequence_img_path)))[::5]:\n",
    "            with open(sequence_img_path + f\"{i:05}.jpg\", \"rb\") as handle:\n",
    "              curr_image = np.array(Image.open(handle))\n",
    "            annotation = model(\n",
    "                first_image, first_annotation, prev_image, prev_annotation, curr_image\n",
    "            )\n",
    "            Image.fromarray(annotation).save(sequence_out_path + f\"{i:05}.png\")\n",
    "\n",
    "            prev_image = curr_image\n",
    "            prev_annoration = first_annotation\n",
    "    return\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"davis2017-evaluation/evaluation_method.py\",\n",
    "            \"--davis_path\",\n",
    "            \"data_storage/DAVIS\",\n",
    "            \"--results_path\",\n",
    "            output_path,\n",
    "            \"--task\",\n",
    "            \"semi-supervised\",\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86076155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "591c33e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_.models import make_model\n",
    "from tensorflow_.fmapicon_utils import execute_model\n",
    "import matplotlib.pyplot as plt\n",
    "l = []\n",
    "\n",
    "class FMAPICON_model:\n",
    "    def __init__(self, weights_path):\n",
    "        self.inner_model = make_model(.6, 90, 128)\n",
    "        self.inner_model.load_weights(weights_path)\n",
    "    def __call__(self, initial_frame, initial_mask, prev_frame, prev_mask, current_frame):\n",
    "        crop = (initial_frame.shape[1] - initial_frame.shape[0]) // 2\n",
    "        B = initial_frame[None, ::4, crop:-crop:4] / 255.   \n",
    "        A = current_frame[None, ::4, crop:-crop:4] / 255.\n",
    "        \n",
    "        cc, grid = execute_model(A, B, self.inner_model, N=1)\n",
    "        m = initial_mask[::4, crop:-crop:4]\n",
    "\n",
    "        shitty_res = m[grid[0, :, :, 1].astype(int) + 15, grid[0, :, :, 0].astype(int) + 15].transpose()\n",
    "\n",
    "        up_res = np.repeat(np.repeat(shitty_res, 4, axis=0),4, axis=1)\n",
    "        \n",
    "        out_mask = initial_mask.copy()\n",
    "        \n",
    "        out_mask[15 * 4:-15 * 4, crop + 15 * 4:-crop - 15 * 4] = up_res\n",
    "        \n",
    "        plt.imshow(A[0, 15:-15, 15:-15])\n",
    "        plt.show()\n",
    "        #plt.imshow(B[0, 15:-15, 15:-15])\n",
    "        #plt.show()\n",
    "        plt.imshow(out_mask)\n",
    "        plt.show()\n",
    "        l.append(up_res)\n",
    "        \n",
    "        plt.imshow(grid[0, :, :, 0].transpose())\n",
    "        plt.show()\n",
    "        \n",
    "        l.append((cc, A, B, initial_mask))\n",
    "        \n",
    "        \n",
    "\n",
    "        return initial_mask\n",
    "\n",
    "#DavisEval(FMAPICON_model(\"tensorflow_/results/log_probability_2/epoch50/model_weights.tf\"), \"FMAPICON\")\n",
    "DavisEval(FMAPICON_model(\"tensorflow_/results/clip.6_11/epoch51/model_weights.tf\"), \"FMAPICON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a13065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'cast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9714/1038132324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDavisEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMAPICON_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch/results/deeeep_warp/network00006.trch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FMAPICON\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9714/1251898731.py\u001b[0m in \u001b[0;36mDavisEval\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_img_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"{i:05}.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m               \u001b[0mcurr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             annotation = model(\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mfirst_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_annotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_annotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             )\n",
      "\u001b[0;32m/tmp/ipykernel_9714/2901450458.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, initial_frame, initial_mask, prev_frame, prev_mask, current_frame)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9714/2901450458.py\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(A, B, model, N)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mFEATURE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeats_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'cast'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdf0703",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21209/745748779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "grid,A,B,m=l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d920d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grid[0, :, :, 0].transpose())\n",
    "plt.show()\n",
    "plt.imshow(A[0])\n",
    "plt.show()\n",
    "plt.imshow(B[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42018c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shitty_res = m[grid[0, :, :, 1].astype(int) + 15, grid[0, :, :, 0].astype(int) + 15].transpose()\n",
    "\n",
    "up_res = np.repeat(np.repeat(shitty_res, 4, axis=0),4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m[15:-15, 15:-15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb74f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24584c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfde0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
