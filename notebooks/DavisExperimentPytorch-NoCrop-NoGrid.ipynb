{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import subprocess\n",
    "import footsteps\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "def DavisEval(model, name, partial=False):\n",
    "    davis_path = \"data_storage/DAVIS/\"\n",
    "    output_path = footsteps.output_dir + name + \"/\"\n",
    "    \n",
    "    os.mkdir(output_path)\n",
    "    with open(davis_path + \"/ImageSets/2017/val.txt\", \"r\") as f:\n",
    "        sequences = f.readlines()\n",
    "    if partial:\n",
    "        sequences = sequences[8:30]\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        sequence = sequence[:-1]  # strip newline\n",
    "        sequence_out_path = output_path + sequence + \"/\"\n",
    "        sequence_img_path = davis_path + \"JPEGImages/480p/\" + sequence + \"/\"\n",
    "        with open(davis_path + \"Annotations/480p/\" + sequence + \"/00000.png\", \"rb\") as handle:\n",
    "          first_annotation = np.array(\n",
    "            Image.open(handle)\n",
    "          )\n",
    "\n",
    "        os.mkdir(output_path + sequence)\n",
    "        with open(sequence_img_path + \"00000.jpg\", \"rb\") as handle:\n",
    "          first_image = np.array(Image.open(handle))\n",
    "\n",
    "        prev_image = first_image\n",
    "        prev_annotation = first_annotation\n",
    "        \n",
    "        if partial:\n",
    "            frames = range(1, len(os.listdir(sequence_img_path)))[::5]\n",
    "        else:\n",
    "            frames = range(1, len(os.listdir(sequence_img_path)))\n",
    "\n",
    "        for i in frames:\n",
    "            \n",
    "            with open(sequence_img_path + f\"{i:05}.jpg\", \"rb\") as handle:\n",
    "              curr_image = np.array(Image.open(handle))\n",
    "            annotation = model(\n",
    "                first_image, first_annotation, prev_image, prev_annotation, curr_image\n",
    "            )\n",
    "            Image.fromarray(annotation).save(sequence_out_path + f\"{i:05}.png\")\n",
    "\n",
    "            prev_image = curr_image\n",
    "            prev_annoration = first_annotation\n",
    "    \n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"davis2017-evaluation/evaluation_method.py\",\n",
    "            \"--davis_path\",\n",
    "            \"data_storage/DAVIS\",\n",
    "            \"--results_path\",\n",
    "            output_path,\n",
    "            \"--task\",\n",
    "            \"semi-supervised\",\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c80f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86076155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fmapicon.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c33e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "l = []\n",
    "\n",
    "def execute_model(A, B, model):\n",
    "    N = A.shape[0]\n",
    "    SIDE_LENGTH = A.shape[2]\n",
    "    \n",
    "    \n",
    "    A = torch.tensor(A).cuda().permute(0, 3, 1, 2).float()[:, [2, 1, 0]]\n",
    "    B = torch.tensor(B).cuda().permute(0, 3, 1, 2).float()[:, [2, 1, 0]]\n",
    "    feats_a = model(A)\n",
    "    feats_b = model(B)\n",
    "\n",
    "    # to (N, C, H*W)\n",
    "    feats_a = feats_a.reshape(feats_a.shape[0],\n",
    "        feats_a.shape[1], feats_a.shape[2] * feats_a.shape[3])\n",
    "    feats_b = feats_b.reshape(feats_b.shape[0],\n",
    "        feats_b.shape[1], feats_b.shape[2] * feats_b.shape[3])\n",
    "\n",
    "    feats_a = feats_a.permute(0, 2, 1)\n",
    "\n",
    "    #return feats_a\n",
    "    cc = torch.bmm(feats_a, feats_b)\n",
    "\n",
    "    cc = nn.functional.softmax(cc, dim=-1)\n",
    "\n",
    "\n",
    "    cc = cc.reshape([N] + [SIDE_LENGTH] * 4)\n",
    "    #cc = np.array(cc.cpu().detach())\n",
    "    \n",
    "    cc2 = cc.reshape([N, SIDE_LENGTH, SIDE_LENGTH, SIDE_LENGTH**2])\n",
    "    \n",
    "    index_grid = torch.argmax(cc2, axis=-1).cpu().detach()[:, :, :, None].numpy()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    grid = np.concatenate([index_grid % SIDE_LENGTH, index_grid / SIDE_LENGTH], axis=-1)\n",
    "    \n",
    "    return cc, grid\n",
    "\n",
    "class FMAPICON_model:\n",
    "    def __init__(self, weights_path, do_vis = False):\n",
    "        self.inner_model = fmapicon.models.tallUNet64().cuda()\n",
    "        self.inner_model.eval()\n",
    "        self.inner_model.load_state_dict(torch.load(weights_path))\n",
    "        self.do_vis = do_vis\n",
    "        \n",
    "    def __call__(self, initial_frame, initial_mask, prev_frame, prev_mask, current_frame):\n",
    "        crop = (initial_frame.shape[1] - initial_frame.shape[0]) // 2\n",
    "        B = initial_frame[None, ::4, crop:-crop:4] / 255.   \n",
    "        A = current_frame[None, ::4, crop:-crop:4] / 255.\n",
    "        \n",
    "        cc, grid = execute_model(A, B, self.inner_model)\n",
    "        m = initial_mask[2::4, 2+crop:-crop:4]\n",
    "\n",
    "        shitty_res = m[grid[0, :, :, 1].astype(int), grid[0, :, :, 0].astype(int)]\n",
    "\n",
    "        up_res = np.repeat(np.repeat(shitty_res, 4, axis=0),4, axis=1)\n",
    "        \n",
    "        out_mask = initial_mask.copy()\n",
    "        \n",
    "        out_mask[:, crop:-crop] = up_res\n",
    "        if self.do_vis:\n",
    "            plt.imshow(A[0])\n",
    "            plt.show()\n",
    "            plt.imshow(B[0])\n",
    "            plt.show()\n",
    "            plt.imshow(shitty_res)\n",
    "            plt.show()\n",
    "            plt.imshow(out_mask)\n",
    "            plt.show()\n",
    "            #l.append(up_res)\n",
    "\n",
    "            plt.imshow(grid[0, :, :, 0])\n",
    "            plt.show()\n",
    "\n",
    "            #l.append((cc.detach().cpu(), A, B, initial_mask))\n",
    "        \n",
    "        \n",
    "\n",
    "        return out_mask\n",
    "\n",
    "#DavisEval(FMAPICON_model(\"tensorflow_/results/log_probability_2/epoch50/model_weights.tf\"), \"FMAPICON\")\n",
    "#DavisEval(FMAPICON_model(\"tensorflow_/results/clip.6_11/epoch51/model_weights.tf\"), \"FMAPICON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = False\n",
    "\n",
    "DavisEval(FMAPICON_model(f\"results/rolling_augmentation_2/network00050.trch\", vis), f\"rolling_test\", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f67208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [01:28<00:06,  3.39s/it]"
     ]
    }
   ],
   "source": [
    "vis = False\n",
    "\n",
    "DavisEval(FMAPICON_model(f\"results/batch_norm_evverrwer/network00080.trch\", vis), f\"bnormv\", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = True\n",
    "DavisEval(FMAPICON_model(f\"results/clip_gradient_mannequin/network00043.trch\", vis), f\"manneq_bgr4\", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46eb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = True\n",
    "DavisEval(FMAPICON_model(f\"results/pixelwise_norm_7/network00086.trch\", vis), f\"dod738od990o\", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed265806",
   "metadata": {},
   "outputs": [],
   "source": [
    "DavisEval(FMAPICON_model(f\"results/kinetics_train/network00005.trch\"), f\"kinetics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FMAPICON_model(f\"results/clip_norm/network00041.trch\").inner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc,A,B,m=l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc, grid = execute_model(A, B, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d920d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fmapicon.training\n",
    "feats_A = fmapicon.training.warping(model, torch.tensor(A).cuda().permute(0, 3, 1, 2).float()).detach().cpu().numpy()\n",
    "feats_B = model(torch.tensor(B).cuda().permute(0, 3, 1, 2).float()).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5796d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(feats_A[0, 7])\n",
    "plt.title(\"Edge artifacts from warping and unwarping.\\n Must be left in or training fails\")\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.title(\"cropped feature map\\n from network trained with cropping\")\n",
    "plt.imshow(feats_A[0, 3, 20:-20, 20:-20])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum(feats_A[0]**2, axis=0))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(feats_A[0 , 18])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(feats_B[0 , 18])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(grid[0, :, :, 0].transpose())\n",
    "#plt.show()\n",
    "plt.imshow(A[0])\n",
    "plt.show()\n",
    "plt.imshow(B[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42018c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71b7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f44a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb74f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24584c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfde0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa860b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
